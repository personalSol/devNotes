{
	"nodes":[
		{"id":"ml-limitation","type":"text","text":"### ‚ö†Ô∏è ML Limitation\n\n**Problem:** Manual feature engineering\n\nHumans had to specify WHAT to look for:\n- \"Look for whiskers\"\n- \"Count edges\"\n- \"Measure texture\"\n\n**Time-consuming & requires expertise**","x":-400,"y":730,"width":300,"height":430,"color":"1"},
		{"id":"title-node","type":"text","text":"# The Complete Mental Map of AI\n## From First Principles to Modern Models","x":-160,"y":-860,"width":600,"height":120,"color":"6"},
		{"id":"ai-root","type":"text","text":"## ü§ñ ARTIFICIAL INTELLIGENCE\n**The Goal:** Machines that can think, learn, and solve problems\n\n*The entire field of making machines intelligent*","x":-60,"y":-660,"width":400,"height":240,"color":"5"},
		{"id":"ml-root","type":"text","text":"## üéØ MACHINE LEARNING\n**Era:** 1980s-2010s\n\n**Breakthrough:** Learn patterns from data instead of explicit rules\n\n**Key Insight:** Show examples, let computer find patterns","x":-50,"y":-280,"width":400,"height":280,"color":"4"},
		{"id":"symbolic-ai","type":"text","text":"### Symbolic AI\n**Era:** 1950s-1990s\n\n**Approach:** Logic & reasoning\n- Expert systems\n- Knowledge graphs\n- Formal logic\n\n**Still used in:** Theorem proving, planning","x":550,"y":-280,"width":320,"height":360,"color":"1"},
		{"id":"rule-based","type":"text","text":"### Rule-Based AI\n**Era:** 1950s-1980s\n\n**Approach:** Write explicit rules\n```\nIF fever AND cough THEN flu\n```\n\n**Problem:** Too rigid, can't scale\n‚ùå Brittle\n‚ùå Can't handle nuance","x":-650,"y":-280,"width":320,"height":360,"color":"1"},
		{"id":"unsupervised","type":"text","text":"### Unsupervised Learning\n**Learning WITHOUT labels**\n\nüîç Find hidden patterns\n\n**Types:**\n- Clustering (grouping)\n- Dimensionality reduction\n\n**Examples:**\n- Customer segmentation\n- Anomaly detection\n- Topic modeling","x":-10,"y":180,"width":300,"height":480,"color":"2"},
		{"id":"reinforcement","type":"text","text":"### Reinforcement Learning\n**Learning by trial & error**\n\nüéÆ Agent + Environment + Rewards\n\nLike training a dog:\n‚úÖ Good action ‚Üí Treat\n‚ùå Bad action ‚Üí Nothing\n\n**Examples:**\n- AlphaGo\n- Game playing\n- Robotics","x":390,"y":180,"width":300,"height":480,"color":"2"},
		{"id":"supervised","type":"text","text":"### Supervised Learning\n**Learning with a teacher**\n\nüìù Examples WITH answers\n- \"This is a cat\" (show image)\n- \"This is a dog\" (show image)\n\n**Types:**\n- Classification (categories)\n- Regression (numbers)\n\n**Examples:** Email spam detection, price prediction","x":-410,"y":180,"width":300,"height":480,"color":"2"},
		{"id":"deep-learning","type":"text","text":"## üß† DEEP LEARNING\n**Era:** 2010s\n**The Revolution:** Multi-layer neural networks\n\n### Key Breakthrough:\n**Learns features AUTOMATICALLY**\n\nNo more manual feature engineering!\n\nRaw Data ‚Üí [Neural Net] ‚Üí Answer","x":260,"y":730,"width":400,"height":430,"color":"3"},
		{"id":"encoder-decoder","type":"text","text":"### üîÑ Encoder-Decoder\n**Purpose:** Transformation\n\n**Architecture:**\nInput ‚Üí Encoder ‚Üí Decoder ‚Üí Output\n\n**Best for:**\n- Translation\n- Summarization\n- Question answering\n- Text-to-text tasks\n\n**Examples:**\n- T5 (2020)\n- BART (2020)\n- mT5","x":180,"y":4760,"width":340,"height":526,"color":"4"},
		{"id":"other-llms","type":"text","text":"### Other Major LLMs\n\n**Gemini (Google)**\n- Multimodal from ground up\n- Text, image, video, audio\n\n**LLaMA (Meta)**\n- Open weights\n- Efficient training\n\n**Mistral**\n- European AI\n- Efficient architecture\n\n**All built on transformers!**","x":180,"y":5530,"width":350,"height":549,"color":"5"},
		{"id":"llm-era","type":"text","text":"## üåü LLM ERA\n**Large Language Models**\n**Era:** 2020 - Present\n\n### What are LLMs?\n**Decoder-only transformers at massive scale**\n\nNot a new architecture!\nJust transformers scaled up:\n- Billions of parameters\n- Trillions of training tokens\n- Instruction-tuned for chat\n\n### The Stack:\n1. Transformer architecture\n2. Scaled to billions of parameters\n3. Trained on internet-scale data\n4. Fine-tuned for instructions","x":680,"y":4760,"width":420,"height":690,"color":"6"},
		{"id":"encoder-only","type":"text","text":"### üìñ Encoder-Only\n**Purpose:** Understanding\n\n**Architecture:**\nText ‚Üí Bidirectional attention ‚Üí Understanding\n\n**Best for:**\n- Text classification\n- Sentiment analysis\n- Named entity recognition\n- Search\n\n**Examples:**\n- BERT (2018)\n- RoBERTa (2019)\n- DistilBERT","x":-720,"y":4760,"width":340,"height":526,"color":"4"},
		{"id":"gpt-family","type":"text","text":"### GPT Family\n**Generative Pre-trained Transformer**\n\n**Evolution:**\n- GPT-2 (2019): 1.5B params\n- GPT-3 (2020): 175B params\n- GPT-4 (2023): ~1.7T params\n\n**Key insight:**\nMore parameters + More data + More compute\n= Emergent abilities\n\n**Applications:**\n- ChatGPT\n- Code generation\n- Creative writing","x":-720,"y":5530,"width":350,"height":549,"color":"5"},
		{"id":"decoder-only","type":"text","text":"### ‚úçÔ∏è Decoder-Only\n**Purpose:** Generation\n\n**Architecture:**\nText ‚Üí Causal attention ‚Üí Next token prediction\n\n**Best for:**\n- Text generation\n- Completion\n- Conversation\n- Creative writing\n\n**Examples:**\n- GPT-2, GPT-3, GPT-4\n- Claude\n- LLaMA, Mistral","x":-270,"y":4760,"width":340,"height":526,"color":"4"},
		{"id":"claude","type":"text","text":"### Claude (Anthropic)\n**Constitutional AI approach**\n\n**Architecture:**\nDecoder-only transformer\n\n**Key features:**\n- Large context window (200K+ tokens)\n- Instruction following\n- Helpful, harmless, honest\n- Constitutional AI training\n\n**Versions:**\n- Claude 1, 2, 3\n- Opus, Sonnet, Haiku tiers","x":-270,"y":5530,"width":350,"height":549,"color":"5"},
		{"id":"neural-network","type":"text","text":"### Neural Networks\n**Inspired by brain neurons**\n\nüîó Layers of connected neurons:\n- Input layer\n- Hidden layers (many = \"deep\")\n- Output layer\n\nEach neuron:\n`output = activation(Œ£(weights √ó inputs) + bias)`","x":-60,"y":1500,"width":320,"height":400,"color":"2"},
		{"id":"rnn","type":"text","text":"### üîÑ RNN\n**Recurrent Neural Network**\n**Era:** 2014-2017\n**Best for:** Sequential data\n\n**How it works:**\nWord 1 ‚Üí Process ‚Üí Memory\nWord 2 ‚Üí Process (+ memory) ‚Üí Updated\nWord 3 ‚Üí Process (+ memory) ‚Üí Updated\n\n**Problems:**\n‚ùå Sequential (slow)\n‚ùå Forgets long context\n‚ùå Vanishing gradient","x":140,"y":2440,"width":320,"height":480,"color":"4"},
		{"id":"transformer-revolution","type":"text","text":"## ‚ö° TRANSFORMER REVOLUTION\n**Era:** 2017 - Present\n**Paper:** \"Attention Is All You Need\"\n### üöÄ The Breakthrough:\n**Self-Attention Mechanism**\n\nProcess ALL words simultaneously\nLet them \"talk\" to each other\n\n**Advantages:**\n‚úÖ Parallel processing (fast)\n‚úÖ Long-range dependencies\n‚úÖ Scalable\n‚úÖ Works for ANY data type","x":-20,"y":3180,"width":500,"height":520,"color":"5"},
		{"id":"attention-mechanism","type":"text","text":"### üëÅÔ∏è Attention Mechanism\n**Core Innovation**\n\n**Old way (RNN):**\nThe ‚Üí cat ‚Üí sat ‚Üí on ‚Üí mat\n(one at a time)\n\n**Transformer way:**\n[The, cat, sat, on, mat]\nAll words processed together\n\"cat\" attends to \"sat\" and \"mat\"\n\n**Mental Model:**\nLike reading with comprehension\n\"The bank was steep\" ‚Üí bank = riverbank\nYou attended to \"steep\"","x":-520,"y":4080,"width":380,"height":500,"color":"3"},
		{"id":"transformer-architecture","type":"text","text":"### üèóÔ∏è Transformer Architecture\n\n**Key Components:**\n1. **Tokenization** - Break text into tokens\n2. **Embeddings** - Convert to vectors\n3. **Positional Encoding** - Add position info\n4. **Self-Attention Layers** - Process relationships\n5. **Feed-Forward Layers** - Transform representations\n\n**Two main structures:**\n- Encoder (understanding)\n- Decoder (generation)","x":180,"y":4080,"width":380,"height":500,"color":"3"},
		{"id":"vision-transformers","type":"text","text":"### üëÅÔ∏è Vision Transformers (ViT)\n**Era:** 2020+\n\n**Revolutionary insight:**\nWhy use CNNs?\nTransformers can do images too!\n\n**How:**\nImage ‚Üí Cut into patches ‚Üí Treat as tokens\n[Patch1][Patch2]...[Patch196]\n‚Üì\nTransformer (like words)\n‚Üì\nImage understanding\n\n**Examples:**\n- CLIP (image-text)\n- DALL-E (text-to-image)\n- Segment Anything (SAM)","x":680,"y":5530,"width":380,"height":610,"color":"4"},
		{"id":"summary","type":"text","text":"## üéì ONE-SENTENCE SUMMARY\n\n**Modern AI is transformers (attention-based neural networks) scaled to billions of parameters and trained on internet-scale data.**\n\n---\n\n## The Mental Map to Hold:\n\n```\nProblem ‚Üí Data ‚Üí Transformer ‚Üí Scale ‚Üí Magic\n```\n\n---\n\n## The Core Insight:\n\n- **AI** = The goal (machine intelligence)\n- **ML** = The method (learning from data)\n- **DL** = The implementation (neural networks)\n- **Transformers** = The winning architecture (attention)\n- **LLMs/ViT/Multimodal** = Scaled transformers applied\n\n**Everything modern is transformers + scale + data.**","x":-80,"y":7640,"width":600,"height":700,"color":"5"},
		{"id":"abstraction","type":"text","text":"## üî¨ ABSTRACTION LADDER\n\n**Level 5 (Abstract):**\nArtificial General Intelligence\n‚Üì\n**Level 4:**\nMachine Learning philosophy\n‚Üì\n**Level 3:**\nDeep Learning method\n‚Üì\n**Level 2:**\nTransformer architecture\n‚Üì\n**Level 1 (Concrete):**\nGPT-4, Claude, BERT\n\n**Read bottom-up:**\nModels are implementations of transformers,\nwhich use deep learning,\nwhich is a type of ML,\nwhich aims toward AGI","x":290,"y":6900,"width":380,"height":640,"color":"1"},
		{"id":"key-insights","type":"text","text":"## üí° KEY MENTAL SHIFTS\n\n**1. Rules ‚Üí Patterns**\nFrom \"tell the computer\" to \"show examples\"\n\n**2. Features ‚Üí Representations**\nFrom manual features to learned features\n\n**3. Sequential ‚Üí Parallel**\nFrom word-by-word to all-at-once\n\n**4. Specialized ‚Üí General**\nFrom separate models to unified architecture\n\n**5. Algorithm ‚Üí Scale**\nFrom clever algorithms to scale + compute + data\n\n**The modern formula:**\nTransformers + Scale + Data = Magic","x":790,"y":6900,"width":420,"height":614,"color":"6"},
		{"id":"timeline","type":"text","text":"## üìÖ TIMELINE VIEW\n\n**1950s-1980s:** Rule-based AI\n‚Üì\n**1980s-2010s:** Classical ML\n‚Üì\n**2006-2012:** Deep learning awakening\n‚Üì\n**2012:** CNN breakthrough (AlexNet)\n‚Üì\n**2014-2017:** RNN/LSTM era\n‚Üì\n**2017:** Transformer revolution ‚ö°\n‚Üì\n**2018-2020:** BERT & GPT-2/3\n‚Üì\n**2020-Present:** LLMs + Multimodal\n\n**Current:** Scaling era","x":-710,"y":6900,"width":380,"height":614,"color":"1"},
		{"id":"problem-solution","type":"text","text":"## üéØ PROBLEM ‚Üí SOLUTION VIEW\n\n**Rules too complex**\n‚Üí Machine Learning (learn patterns)\n\n**Manual features needed**\n‚Üí Deep Learning (learn features)\n\n**Images hard to process**\n‚Üí CNNs (convolution)\n\n**Sequences processed slowly**\n‚Üí RNNs (memory)\n\n**Short memory, sequential**\n‚Üí Transformers (attention)\n\n**Single modality limiting**\n‚Üí Multimodal (unified tokens)","x":-210,"y":6900,"width":380,"height":614,"color":"1"},
		{"id":"multimodal","type":"text","text":"## üé® MULTIMODAL MODELS\n**Era:** 2022 - Present\n\n### The Unification\n**Everything becomes tokens**\n\nText: \"Hello\" ‚Üí [Token: 123]\nImage patch ‚Üí [Token: 456]\nAudio segment ‚Üí [Token: 789]\n\nTransformer doesn't care about origin!\n\n**Examples:**\n- GPT-4V (GPT-4 with vision)\n- Gemini (native multimodal)\n- CLIP (image-text)\n\n**Key insight:**\nOne architecture, all modalities","x":-20,"y":6160,"width":500,"height":640,"color":"6"},
		{"id":"cnn","type":"text","text":"### üì∏ CNN\n**Convolutional Neural Network**\n**Era:** 2012\n**Best for:** Images\n\n**How it thinks:**\n1. Layer 1: Detect edges\n2. Layer 2: Combine into shapes\n3. Layer 3: Form parts (eyes, ears)\n4. Layer 4: Recognize objects (cat)\n\n**Breakthrough:** AlexNet (2012)","x":-380,"y":2440,"width":320,"height":452,"color":"4"},
		{"id":"lstm","type":"text","text":"### üß© LSTM\n**Long Short-Term Memory**\n**Era:** 2015-2017\n**Improvement over RNN**\n\n**Innovation:** Memory gates\n- Forget gate (what to discard)\n- Input gate (what to store)\n- Output gate (what to output)\n\n**Still limited:**\n‚ùå Sequential processing\n‚ùå Limited context window","x":760,"y":2440,"width":320,"height":480,"color":"4"}
	],
	"edges":[
		{"id":"edge-1","fromNode":"ai-root","fromSide":"bottom","toNode":"ml-root","toSide":"top","label":"evolved into"},
		{"id":"edge-2","fromNode":"ai-root","fromSide":"left","toNode":"rule-based","toSide":"top","label":"first attempt"},
		{"id":"edge-3","fromNode":"ai-root","fromSide":"right","toNode":"symbolic-ai","toSide":"top","label":"parallel path"},
		{"id":"edge-4","fromNode":"ml-root","fromSide":"bottom","toNode":"supervised","toSide":"top"},
		{"id":"edge-5","fromNode":"ml-root","fromSide":"bottom","toNode":"unsupervised","toSide":"top"},
		{"id":"edge-6","fromNode":"ml-root","fromSide":"bottom","toNode":"reinforcement","toSide":"top"},
		{"id":"edge-7","fromNode":"supervised","fromSide":"bottom","toNode":"ml-limitation","toSide":"top","label":"limitation"},
		{"id":"edge-8","fromNode":"ml-limitation","fromSide":"right","toNode":"deep-learning","toSide":"left","label":"solved by"},
		{"id":"edge-9","fromNode":"deep-learning","fromSide":"bottom","toNode":"neural-network","toSide":"top","label":"foundation"},
		{"id":"edge-10","fromNode":"neural-network","fromSide":"bottom","toNode":"cnn","toSide":"top","label":"specialized for images"},
		{"id":"edge-11","fromNode":"neural-network","fromSide":"bottom","toNode":"rnn","toSide":"top","label":"specialized for sequences"},
		{"id":"edge-12","fromNode":"rnn","fromSide":"right","toNode":"lstm","toSide":"left","label":"improved to"},
		{"id":"edge-13","fromNode":"lstm","fromSide":"bottom","toNode":"transformer-revolution","toSide":"top","label":"replaced by"},
		{"id":"edge-14","fromNode":"transformer-revolution","fromSide":"bottom","toNode":"attention-mechanism","toSide":"top","label":"core innovation"},
		{"id":"edge-15","fromNode":"transformer-revolution","fromSide":"bottom","toNode":"transformer-architecture","toSide":"top","label":"architecture"},
		{"id":"edge-16","fromNode":"transformer-architecture","fromSide":"bottom","toNode":"encoder-only","toSide":"top","label":"variant"},
		{"id":"edge-17","fromNode":"transformer-architecture","fromSide":"bottom","toNode":"decoder-only","toSide":"top","label":"variant"},
		{"id":"edge-18","fromNode":"transformer-architecture","fromSide":"bottom","toNode":"encoder-decoder","toSide":"top","label":"variant"},
		{"id":"edge-19","fromNode":"decoder-only","fromSide":"right","toNode":"llm-era","toSide":"left","label":"scaled up"},
		{"id":"edge-20","fromNode":"llm-era","fromSide":"bottom","toNode":"gpt-family","toSide":"top"},
		{"id":"edge-21","fromNode":"llm-era","fromSide":"bottom","toNode":"claude","toSide":"top"},
		{"id":"edge-22","fromNode":"llm-era","fromSide":"bottom","toNode":"other-llms","toSide":"top"},
		{"id":"edge-23","fromNode":"transformer-architecture","fromSide":"right","toNode":"vision-transformers","toSide":"left","label":"applied to images"},
		{"id":"edge-24","fromNode":"gpt-family","fromSide":"bottom","toNode":"multimodal","toSide":"left"},
		{"id":"edge-25","fromNode":"vision-transformers","fromSide":"bottom","toNode":"multimodal","toSide":"right"},
		{"id":"edge-26","fromNode":"multimodal","fromSide":"bottom","toNode":"summary","toSide":"top","label":"culminates in"}
	]
}